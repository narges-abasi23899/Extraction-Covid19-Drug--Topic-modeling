{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "85lBdk49SvvG"
      },
      "outputs": [],
      "source": [
        "# Run in terminal or command prompt\n",
        "# python3 -m spacy download en\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, nltk, spacy, gensim\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pprint import pprint\n",
        "\n",
        "# Plotting tools\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dataset\n",
        "df = pd.read_json('Dataset-Drug.json')\n",
        "df.head(15)"
      ],
      "metadata": {
        "id": "lgPqePt8S5Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to list\n",
        "data = df.content.values.tolist()\n",
        "\n",
        "# Remove Emails\n",
        "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
        "\n",
        "# Remove new line characters\n",
        "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
        "#\n",
        "data = [re.sub (\"(from|re|edu|use|that|covid|corona|virus|coronavirus|drug)\",\"\", sent) for sent in data]\n",
        "\n",
        "pprint(data[:2])"
      ],
      "metadata": {
        "id": "BZd5zYM2_wdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sentence in sentences:\n",
        "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
        "\n",
        "data_words = list(sent_to_words(data))\n",
        "\n",
        "print(data_words[:1])"
      ],
      "metadata": {
        "id": "ZF02DU54TEMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run in python console\n",
        "import nltk; nltk.download('stopwords')\n",
        "\n",
        "# Run in terminal or command prompt\n",
        "import spacy\n",
        "!python3 -m spacy download 'en_core_web_sm'"
      ],
      "metadata": {
        "id": "YFwwjD0pAwNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
        "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
        "    texts_out = []\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent))\n",
        "        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n",
        "    return texts_out\n",
        "\n",
        "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
        "# Run in terminal: python3 -m spacy download ennlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp.disable_pipe(\"parser\")\n",
        "nlp.enable_pipe(\"senter\")\n",
        "# Do lemmatization keeping only Noun, Adj, Verb, Adverb\n",
        "data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "print(data_lemmatized[:2])"
      ],
      "metadata": {
        "id": "DCUGgom2THah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(analyzer='word',\n",
        "                             min_df=10,                        # minimum reqd occurences of a word\n",
        "                             stop_words='english',             # remove stop words\n",
        "                             lowercase=True,                   # convert all words to lowercase\n",
        "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
        "                             # max_features=50000,             # max number of uniq words\n",
        "                            )\n",
        "\n",
        "data_vectorized = vectorizer.fit_transform(data_lemmatized)"
      ],
      "metadata": {
        "id": "cnop1EGMTLXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Materialize the sparse data\n",
        "data_dense = data_vectorized.todense()\n",
        "\n",
        "# Compute Sparsicity = Percentage of Non-Zero cells\n",
        "print(\"Sparsicity: \", ((data_dense > 0).sum()/data_dense.size)*100, \"%\")"
      ],
      "metadata": {
        "id": "TDvAYD7lTTn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build LDA Model\n",
        "lda_model = LatentDirichletAllocation(n_components=5,               # Number of topics\n",
        "                                      max_iter=10,               # Max learning iterations\n",
        "                                      learning_method='online',\n",
        "                                      random_state=100,          # Random state\n",
        "                                      batch_size=128,            # n docs in each learning iter\n",
        "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
        "                                      n_jobs = -1,               # Use all available CPUs\n",
        "                                     )\n",
        "lda_output = lda_model.fit_transform(data_vectorized)\n",
        "\n",
        "print(lda_model)  # Model attributes"
      ],
      "metadata": {
        "id": "FkL3CqMYTXSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
        "             evaluate_every=-1, learning_decay=0.7,\n",
        "             learning_method='online', learning_offset=10.0,\n",
        "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
        "             n_components=5, n_jobs=-1, perp_tol=0.1,\n",
        "             random_state=100, topic_word_prior=None,\n",
        "             total_samples=1000000.0, verbose=0)"
      ],
      "metadata": {
        "id": "J_-QtUgVTbIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log Likelyhood: Higher the better\n",
        "print(\"Log Likelihood: \", lda_model.score(data_vectorized))\n",
        "\n",
        "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
        "print(\"Perplexity: \", lda_model.perplexity(data_vectorized))\n",
        "\n",
        "# See model parameters\n",
        "pprint(lda_model.get_params())"
      ],
      "metadata": {
        "id": "diPURTMoTgZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Search Param\n",
        "search_params = {'n_components': [2,3,4,5,6,7,8,9,10], 'learning_decay': [.5, .7, .9]}\n",
        "\n",
        "# Init the Model\n",
        "lda = LatentDirichletAllocation()\n",
        "\n",
        "# Init Grid Search Class\n",
        "model = GridSearchCV(lda, param_grid=search_params)\n",
        "\n",
        "# Do the Grid Search\n",
        "model.fit(data_vectorized)"
      ],
      "metadata": {
        "id": "PcQK9rubTkWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchCV(cv=None, error_score='raise',\n",
        "       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
        "             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
        "             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
        "             mean_change_tol=0.001, n_components=5, n_jobs=1,\n",
        "             perp_tol=0.1, random_state=None,\n",
        "             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
        "             n_jobs=1,\n",
        "       param_grid={'n_topics': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
        "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
        "       scoring=None, verbose=0)"
      ],
      "metadata": {
        "id": "bRa9y-PLTo0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best Model\n",
        "best_lda_model = model.best_estimator_\n",
        "\n",
        "# Model Parameters\n",
        "print(\"Best Model's Params: \", model.best_params_)\n",
        "\n",
        "# Log Likelihood Score\n",
        "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
        "\n",
        "# Perplexity\n",
        "print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))"
      ],
      "metadata": {
        "id": "9pFaEvCETq43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.cv_results_"
      ],
      "metadata": {
        "id": "OWKHLMa1F-Zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Log Likelyhoods from Grid Search Output\n",
        "n_components = [2,3,4,5,6,7,8,9,10]\n",
        "results = pd.DataFrame(model.cv_results_)\n",
        "log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.results if gscore.parameters['learning_decay']==0.5]\n",
        "log_likelyhoods_7 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.7]\n",
        "log_likelyhoods_9 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.9]\n",
        "\n",
        "# Show graph\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(n_components, log_likelyhoods_5, label='0.5')\n",
        "plt.plot(n_components, log_likelyhoods_7, label='0.7')\n",
        "plt.plot(n_components, log_likelyhoods_9, label='0.9')\n",
        "plt.title(\"Choosing Optimal LDA Model\")\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Log Likelyhood Scores\")\n",
        "plt.legend(title='Learning decay', loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7BI5BfymUQcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Document - Topic Matrix\n",
        "lda_output = best_lda_model.transform(data_vectorized)\n",
        "\n",
        "# column names\n",
        "topicnames = [\"Topic\" + str(i) for i in range(best_lda_model.n_topics)]\n",
        "\n",
        "# index names\n",
        "docnames = [\"Doc\" + str(i) for i in range(len(data))]\n",
        "\n",
        "# Make the pandas dataframe\n",
        "df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
        "\n",
        "# Get dominant topic for each document\n",
        "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
        "df_document_topic['dominant_topic'] = dominant_topic\n",
        "\n",
        "# Styling\n",
        "def color_green(val):\n",
        "    color = 'green' if val > .1 else 'black'\n",
        "    return 'color: {col}'.format(col=color)\n",
        "\n",
        "def make_bold(val):\n",
        "    weight = 700 if val > .1 else 400\n",
        "    return 'font-weight: {weight}'.format(weight=weight)\n",
        "\n",
        "# Apply Style\n",
        "df_document_topics = df_document_topic.head(15).style.applymap(color_green).applymap(make_bold)\n",
        "df_document_topics"
      ],
      "metadata": {
        "id": "BI8HMZ-9UZy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\n",
        "df_topic_distribution.columns = ['Topic Num', 'Num Documents']\n",
        "df_topic_distribution"
      ],
      "metadata": {
        "id": "EHF3lNdwIA_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyLDAvis.enable_notebook()\n",
        "panel = pyLDAvis.sklearn.prepare(best_lda_model, data_vectorized, vectorizer, mds='tsne')\n",
        "panel"
      ],
      "metadata": {
        "id": "5xqrdIwaIE1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic-Keyword Matrix\n",
        "df_topic_keywords = pd.DataFrame(best_lda_model.components_)\n",
        "\n",
        "# Assign Column and Index\n",
        "df_topic_keywords.columns = vectorizer.get_feature_names()\n",
        "df_topic_keywords.index = topicnames\n",
        "\n",
        "# View\n",
        "df_topic_keywords.head()"
      ],
      "metadata": {
        "id": "KyI3O7CNIIDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show top n keywords for each topic\n",
        "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=10):\n",
        "    keywords = np.array(vectorizer.get_feature_names())\n",
        "    topic_keywords = []\n",
        "    for topic_weights in lda_model.components_:\n",
        "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
        "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
        "    return topic_keywords\n",
        "\n",
        "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=10)\n",
        "\n",
        "# Topic - Keywords Dataframe\n",
        "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
        "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
        "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
        "df_topic_keywords\n"
      ],
      "metadata": {
        "id": "Mwmx4atFILqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function to predict topic for a given text document.\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "def predict_topic(text, nlp=nlp):\n",
        "    global sent_to_words\n",
        "    global lemmatization\n",
        "\n",
        "    # Step 1: Clean with simple_preprocess\n",
        "    mytext_2 = list(sent_to_words(text))\n",
        "\n",
        "    # Step 2: Lemmatize\n",
        "    mytext_3 = lemmatization(mytext_2, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "\n",
        "    # Step 3: Vectorize transform\n",
        "    mytext_4 = vectorizer.transform(mytext_3)\n",
        "\n",
        "    # Step 4: LDA Transform\n",
        "    topic_probability_scores = best_lda_model.transform(mytext_4)\n",
        "    topic = df_topic_keywords.iloc[np.argmax(topic_probability_scores), :].values.tolist()\n",
        "    return topic, topic_probability_scores\n",
        "\n",
        "# Predict the topic\n",
        "mytext = [\"Some text about christianity and bible\"]\n",
        "topic, prob_scores = predict_topic(text = mytext)\n",
        "print(topic)"
      ],
      "metadata": {
        "id": "vh5DbZZCIQNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the k-means clusters\n",
        "from sklearn.cluster import KMeans\n",
        "clusters = KMeans(n_clusters=15, random_state=100).fit_predict(lda_output)\n",
        "\n",
        "# Build the Singular Value Decomposition(SVD) model\n",
        "svd_model = TruncatedSVD(n_components=2)  # 2 components\n",
        "lda_output_svd = svd_model.fit_transform(lda_output)\n",
        "\n",
        "# X and Y axes of the plot using SVD decomposition\n",
        "x = lda_output_svd[:, 0]\n",
        "y = lda_output_svd[:, 1]\n",
        "\n",
        "# Weights for the 15 columns of lda_output, for each component\n",
        "print(\"Component's weights: \\n\", np.round(svd_model.components_, 2))\n",
        "\n",
        "# Percentage of total information in 'lda_output' explained by the two components\n",
        "print(\"Perc of Variance Explained: \\n\", np.round(svd_model.explained_variance_ratio_, 2))"
      ],
      "metadata": {
        "id": "1oxznDtQIX7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.scatter(x, y, c=clusters)\n",
        "plt.xlabel('Component 2')\n",
        "plt.xlabel('Component 1')\n",
        "plt.title(\"Segregation of Topic Clusters\", )"
      ],
      "metadata": {
        "id": "GK9MQryLIg9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "\n",
        "def similar_documents(text, doc_topic_probs, documents = data, nlp=nlp, top_n=5, verbose=False):\n",
        "    topic, x  = predict_topic(text)\n",
        "    dists = euclidean_distances(x.reshape(1, -1), doc_topic_probs)[0]\n",
        "    doc_ids = np.argsort(dists)[:top_n]\n",
        "    if verbose:\n",
        "        print(\"Topic KeyWords: \", topic)\n",
        "        print(\"Topic Prob Scores of text: \", np.round(x, 1))\n",
        "        print(\"Most Similar Doc's Probs:  \", np.round(doc_topic_probs[doc_ids], 1))\n",
        "    return doc_ids, np.take(documents, doc_ids)"
      ],
      "metadata": {
        "id": "YsdJIpw4IoXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get similar documents\n",
        "mytext = [\"Some text about christianity and bible\"]\n",
        "doc_ids, docs = similar_documents(text=mytext, doc_topic_probs=lda_output, documents = data, top_n=1, verbose=True)\n",
        "print('\\n', docs[0][:500])"
      ],
      "metadata": {
        "id": "e2v26QGRIsZc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}