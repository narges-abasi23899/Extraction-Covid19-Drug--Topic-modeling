{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer # for stemming\n",
        "from nltk.stem import WordNetLemmatizer # for lemmatization\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "from sklearn.model_selection import train_test_split , cross_val_score,KFold\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn import metrics\n",
        "import warnings\n",
        "!pip install imblearn\n",
        "import imblearn\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "JJTrHjw1UEGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChEcPl1lOaL8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import transformers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset\n",
        "from transformers import BertForSequenceClassification\n",
        "!pip install numba & conda install cudatoolkit"
      ],
      "metadata": {
        "id": "wWS3FI8RhCR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "d8eipsBbhIKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_fwf('train.txt', sep=\" \", header=None, names=[ \"id\", \"label\", \"tweet\"] , encoding='UTF-8')\n"
      ],
      "metadata": {
        "id": "KpEF5bPKhP5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape\n",
        "data.head(100)"
      ],
      "metadata": {
        "id": "P4Qu5T80ha9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['label'].unique()"
      ],
      "metadata": {
        "id": "IbF2yWsChd5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "fEJtKLvshhLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = data[['label', 'tweet']]\n",
        "new_df.head()"
      ],
      "metadata": {
        "id": "5rDAC2XAiCWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n"
      ],
      "metadata": {
        "id": "Y_hC6VOAttW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 256\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "# EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', truncation=True, do_lower_case=True)"
      ],
      "metadata": {
        "id": "NtW_OnS9iK7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentData(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.text = dataframe.tweet\n",
        "        self.targets = self.data.label\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.text[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }"
      ],
      "metadata": {
        "id": "yE6ZHS-yiiYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.85\n",
        "train_data=new_df.sample(frac=train_size,random_state=200)\n",
        "validation_data=new_df.drop(train_data.index).reset_index(drop=True)\n",
        "train_data = train_data.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(new_df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"validation Dataset: {}\".format(validation_data.shape))\n",
        "training_set = SentimentData(train_data, tokenizer, MAX_LEN)\n",
        "testing_set = SentimentData(validation_data, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "jzBdShypint_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': False,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)"
      ],
      "metadata": {
        "id": "j2U0rnkLirNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertClass, self).__init__()\n",
        "        self.l1 = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.classifier = torch.nn.Linear(768, 5)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.ReLU()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output"
      ],
      "metadata": {
        "id": "HuiBuERei2cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "id": "OixXJGTPsH94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertClass()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "4_lcTP0_jVbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the loss function and optimizer\n",
        "loss_function = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "uXaibk4si--B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score,recall_score,accuracy_score, f1_score\n",
        "def show_confusion(label,pred_label,nm):\n",
        "  class_names=['No-Addiction','Addiction']\n",
        "  cm=confusion_matrix(label,pred_label)\n",
        "  disp =ConfusionMatrixDisplay(cm,display_labels=class_names)\n",
        "  disp.plot()\n",
        "  plt.title(f'Confusion Matrix {nm}')\n",
        "  plt.xlabel('Actual')\n",
        "  plt.ylabel('Predicted')\n",
        "  plt.show()\n",
        "\n",
        "def mymetrics(Y,P , alg = None):\n",
        "    recall= recall_score(Y, P)\n",
        "    acc2= accuracy_score(Y, P)\n",
        "    precision= precision_score(Y, P)\n",
        "    Fmeasure = f1_score(Y, P)\n",
        "    print(f\"accuracy = {acc2*100}\")\n",
        "    print(f'recall= {recall*100}')\n",
        "    print(f'precision= {precision*100}')\n",
        "    print(f'Fmeasure= {Fmeasure*100}')"
      ],
      "metadata": {
        "id": "grSLd6VMUdk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcuate_accuracy(preds, targets):\n",
        "    n_correct = (preds==targets).sum().item()\n",
        "    return n_correct"
      ],
      "metadata": {
        "id": "KTz4PNBUr44r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
        "\n",
        "def train(epoch):\n",
        "    tr_loss = 0\n",
        "    n_correct = 0\n",
        "    nb_tr_steps = 0\n",
        "    nb_tr_examples = 0\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        tr_loss += loss.item()\n",
        "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
        "        n_correct += calcuate_accuracy(big_idx, targets)\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples+=targets.size(0)\n",
        "\n",
        "        if _%5000==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            accu_step = (n_correct*100)/nb_tr_examples\n",
        "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
        "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # # When using GPU\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
        "    epoch_loss = tr_loss/nb_tr_steps\n",
        "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
        "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
        "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
        "\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "Vmot8_rvsfBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "nokl_xwJsqvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'saved_weights.pt')"
      ],
      "metadata": {
        "id": "rqH4HZRGIODG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "def valid(model, testing_loader):\n",
        "   # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables\n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "  all_logits=[]\n",
        "  all_labels=[]\n",
        "  # Evaluate data for one epoch\n",
        "  for _,data in tqdm(enumerate(testing_loader, 0)):\n",
        "    ids = data['ids'].to(device, dtype = torch.long)\n",
        "    mask = data['mask'].to(device, dtype = torch.long)\n",
        "    token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "    targets = data['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      output = model(ids, mask, token_type_ids)\n",
        "      logits = output\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = targets.to('cpu').numpy()\n",
        "    all_logits.extend(np.argmax(logits, axis=1).flatten())\n",
        "    all_labels.extend(label_ids)\n",
        "  accuracy = accuracy_score(all_logits , all_labels)\n",
        "\n",
        "\n",
        "  val_f1 = f1_score(all_logits, all_labels)\n",
        "  val_precision = precision_score(all_logits, all_labels)\n",
        "  val_recall = recall_score(all_logits, all_labels)\n",
        "\n",
        "  show_confusion(all_logits, all_labels,'Bert')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  y = classification_report(all_labels,all_logits,digits=4)\n",
        "  print(y)"
      ],
      "metadata": {
        "id": "KttiqP2Lst5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"*****validation acuracy*******\")\n",
        "valid(model, testing_loader)"
      ],
      "metadata": {
        "id": "Yp5J34A3sw81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data2 = pd.read_fwf('test.txt', sep=\" \", header=None, names=[ \"id\", \"label\", \"tweet\"] , encoding='UTF-8')\n"
      ],
      "metadata": {
        "id": "MVxe_Ll9szIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data2 = test_data2[['label', 'tweet']]\n",
        "test_data2.head()"
      ],
      "metadata": {
        "id": "a5bJ8hLUs2un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testing_set2 = SentimentData(test_data2, tokenizer, MAX_LEN)\n",
        "testing_loader2 = DataLoader(testing_set2, **test_params)\n",
        "\n",
        "\n",
        "print(\"*****test acuracy*******\")\n",
        "valid(model, testing_loader2)"
      ],
      "metadata": {
        "id": "7sB1Yk17s6_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}